{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树基本理论和简单算法\n",
    "\n",
    "> 本文档主要就目前简单的决策树原理和算法进行简单介绍\n",
    "> 其中包括**决策树，集成学习中的Bagging：随机森林 和 Boosting：GBDT**\n",
    "\n",
    "基本参考的是**numpy-ml**中内容进行加工整理，写这个的目的在于通过自己的角度对整个算法进行梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写在最前面的话：从陈天奇对xgboost进行解释的文章中，提到了机器学习的主要组建：**模型，参数，目标函数**，本文档打算在此基础上进行阐述，本质上而言就是在说**用什么模型**，模型是有很多**参数**构成，但是我们还不知道**如何**去确定这些参数，因此需要**目标函数**这种**评价的手段**进行寻找参数来找到我们认为比较不错的模型\n",
    "\n",
    "## 理论基础\n",
    "\n",
    "(1). 关于什么是决策树？简单直观点看的话就是人们对于“如果。。。。。。那么的”程序化利用\n",
    "\n",
    "(2). 这里的**模型**显然就是**决策树**。**目标**如何从找到好的决策树，这个好就是指的决策树跑完之后我的叶子节点的数据很多都是具有一样的属性或者‘纯度’，这个时候的好坏评判标准就会有差异如果目标变量是**离散值**则可以为信息熵增益最大（ID3），信息熵增益速率（C4.5），gini不纯度来衡量（CART）如果目前变量是**连续值**，则可以用均方误差来衡量，无论采用何种指标最后都会回到**不纯度的衰减幅度上**\n",
    "\n",
    "\n",
    "$\\Delta(L) = L(parent) - P_{left}*L(Left\\quad child) - P_{right}*L(Right\\quad child)$\n",
    "\n",
    "差别就是我们这个纯度的不同定义罢了，其中$P_{left}$指的是相应叶节点占的总的比例\n",
    "\n",
    "对于**离散值**定义有信息熵增益和基尼系数的评价指标\n",
    "\n",
    "**其中信息熵**定义为$-\\sum(P(w))logP(w)$ \n",
    "\n",
    "**其中gini系数**定义为$-\\sum(P(w)*(1-P(w)) = 1 - \\sum(P^2(w))$ 有一种解释是gini系数是信息熵的一阶泰勒展开\n",
    "\n",
    "> 为什么要用gini系数，因为gini系数没有涉及到对数的运算，这样处理简化了模型\n",
    "\n",
    "\n",
    "(3). 决策树的过程：我们需要找到最佳的分割点进行分割，然后继续分割。因为算法的参数包括，**分割的变量，和分割点**\n",
    "\n",
    "20世纪80年代澳大利亚科学家昆兰将信息熵引入了决策树之后，决策树理论才算做有了比较长足的进步和发展，昆兰首先提出了ID3的算法，**只处理离散值**，而且这里的决策树是**不是二叉树**，原文中就有三个叶节点，C4.5是ID3的改进版本，可以处理连续值。CART树的是采用**二叉树**的结构，可以处理离散值（gini）和连续值（mse），其处理连续值是叶子的值可以用平均值来表示。这是这些算法的主要区别\n",
    "\n",
    "## 理论发展：随机森林，GBDT，XGBOOST\n",
    "\n",
    "**随机森林**：有放回的从总样本的进行抽样，使用决策数进行决策，最后选择**投票或者均值**来进行决策\n",
    "\n",
    "**GBDT**：简单而言通过像练习高尔夫球一样去，将上次决策树训练的结果差距作为输入进行下一次（颗）决策树的训练，通过一阶的泰勒的展开来寻找参数，通过将所有决策树结果进行**求和**的运算来进行对函数进行逼近，也就是\n",
    "\n",
    "$\\sum_{k=1}^{K}f_k(X_i)$\n",
    "\n",
    "**XGBOOST**：GBDT的高阶版本，引入了正则项和二阶泰勒展开\n",
    "\n",
    "对于这三种算法其实都是集成学习的范畴，就是我有很多学习器进行学习，如何组织这些学习器便形成了我们所说的**bagging**和**boosting**，代表分别就是**随机森林**和**GBDT，XGBOOST**，从学习器的组织方式来看前者属于**并行**后者属于**串行**，从偏差和方差的角度看，前者属于**高偏差，低方差**，因为学习器都是独立的组合，后者属于**低偏差，高方差**因为后面的学习器的输入是前面的输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.决策树算法的代码实现\n",
    "\n",
    "(1). 由于我们首先提到了信息熵，gini，mse等评价指标我们可以定义如下的函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist/len(y)\n",
    "    return -np.sum(p*np.log(p) for p in ps if p> 0)\n",
    "\n",
    "def gini(y):\n",
    "    hisr = np.bincount(y)\n",
    "    ps = hist/len(y)\n",
    "    return 1- np.sum(p**2 for p in ps)\n",
    "\n",
    "def mse(y):\n",
    "    return np.mean((y - np.mean(y))**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2). 具体运算过程，涉及类的处理，回头看类的定义，虽然不见的这样写类有多么的优雅但是其中类的嵌套和递归的方法还是值的我自己去好好看和学习的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Node:\n",
    "    def __init__(self,left,right,rule):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feature = rule[0]\n",
    "        self.threshold = rule[1]\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self,value):\n",
    "        self.value = value\n",
    "        \n",
    "#定义好了节点和叶子之后开始进行决策树类的编写，涉及到许多方法，这段对于我而言其实有点搞\n",
    "#对于决策数类的定义这块，可以考虑流程化的角度去看，刚才定义的节点和叶子的时候，其实有点像那种最小化知识的思考\n",
    "#对于刚开始的决策树设计流程而言的话，就假设我是开始弄决策树了，我刚开始的深度肯定为0，根节点还没有。为了开始进行树的生长，我需要选择选择选择的变量，\n",
    "#按照什么标准进行分裂，最大深度是多少，计算叶子的时候，是按照目标是分类变量还是连续值，连续值的话可输出值，离散值的话输出概率，基本就是这思路，如果这样\n",
    "#想的话其实整个程序的思路还是比较清楚的，说白了这个的初始化就是说\n",
    "class DecisionTree:\n",
    "    def __init__(self,classifier =True,max_depth=None,n_feats=None,criterion='entropy',seed=None,):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        self.depth = 0\n",
    "        self.root = None\n",
    "        \n",
    "        self.n_feats = n_feats\n",
    "        self.criterion = criterion\n",
    "        self.classifier = classifier\n",
    "        self.max_depth = max_depth if max_depth else np.inf\n",
    "        \n",
    "        if not classifier and criterion in ['gini','entropy']:\n",
    "            raise ValueError(\n",
    "                \"{} is a valid criterion only when classifier = True.\".format(criterion)\n",
    "            )\n",
    "        if classifier and criterion == 'mse':\n",
    "            raise ValueError(\"'mse' is a valid criterion only when classifier = Flase.\")\n",
    "            \n",
    "    def fit(self,X,Y):\n",
    "        self.n_classes = max(Y) + 1 if self.classifier else None\n",
    "        self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats,X.shape[1])\n",
    "        self.root=self._grow(X,Y)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        return np.array([self._traverse(x,self.root) for x in X])\n",
    "    \n",
    "    def predict_class_probs(self,X):\n",
    "        assert self.classifier,\"'predict_calss_probs' undefined for classifier = False\"\n",
    "        return np.array([self._traverse(x,self.root,prob=True) for x in X])\n",
    "#下面的函数才基本上是整个决策树的关键，涉及到了树的生长分割，计算信息增益等基本的计算，不过由于类的饮用稍微还是有点绕\n",
    "#在写这个程序的时候发现作者有个习惯，就是喜欢将那些异常的特殊的情况进行讨论，然后再看正常的情况\n",
    "#要看数据的生长那先看那些情况下我们不进行分割，一个是我分组比较完美，可以分的干净，另一个是达到我最大的学习深度了\n",
    "#这个grow的数据结构是什么呢？目前有点看的不是很懂，或者说返回的值是怎样的结构。因为我的理解这个node只是包含了最后叶节点的分割点和阈值，\n",
    "#那之前的信息在哪里呢？关于这个点我自己表示还没有理解清楚\n",
    "    def _grow(self,X,Y):\n",
    "        if len(set(Y))==1:\n",
    "            if self.classifer:\n",
    "                prob = np.zeros(self.n_classes)\n",
    "                prob[Y[0]] = 1.0\n",
    "            return Leaf(prob) if self.classifer else Leaf(Y[0])\n",
    "        \n",
    "        if self.depth >= self.max_depth:\n",
    "            v = np.mean(Y,axis=0)\n",
    "            if self.classifier:\n",
    "                v = np.bincount(Y,minlength=self.n_classes) / len(Y)\n",
    "            return Leaf(v)\n",
    "#特殊的情况处理好了之后便开始进行树的生长        \n",
    "        N,M = X.shape\n",
    "        self.depth += 1\n",
    "        feat_idxs = np.random.choice(M,self.n_feats,replace=True)\n",
    "        \n",
    "        feat,thresh = self._segment(X,Y,feat_idxs)\n",
    "        l = np.argwhere(X[:,feat] <= thresh).flatten()\n",
    "        r = np.argwhere(X[:,feat] > thresh).flatten()\n",
    "        \n",
    "#这个转换好不错，我还反应了下，个人觉得好不错,而且还是类的递归\n",
    "        left = self._grow(X[l,:],Y[l])\n",
    "        right = self._grow(X[r,:],Y[r])\n",
    "        return Node(left,right,(feat,thresh))\n",
    "\n",
    "    def _segment(self,X,Y,feat_idxs):\n",
    "        best_gain = -np.inf\n",
    "        split_idx,split_thresh = None,None\n",
    "        for i in feat_idxs:\n",
    "            vals = X[:,i]\n",
    "            levels = np.unique(vals)\n",
    "            thresholds = (levels[:-1]+levels[1:])/2 \n",
    "            gains = np.array([self._impurity_gain(Y,t,vals) for t in thresholds])\n",
    "#关于为什么要这样设置阈值，我都不知道为什么，不过也可以说出个大概来，就是对特征变量的值进行去重排序，取中间值进行阈值的筛选\n",
    "            if gains.max() > best_gain:\n",
    "                split_idx = i\n",
    "                best_gain = gains.max()\n",
    "                split_thresh = thresholds[gains.argmax()]\n",
    "        return split_idx,split_thresh\n",
    "#计算信息增益，分解下来就是简单的母节点的信息熵 - 叶节点的信息熵    \n",
    "    def _impurity_gain(self,Y,split_thresh,feat_values):\n",
    "        if self.criterion == \"entropy\":\n",
    "            loss = entropy\n",
    "        elif self.criterion == \"gini\":\n",
    "            loss = gini\n",
    "        elif self.criterion == \"mse\":\n",
    "            loss = mse\n",
    "        \n",
    "        parent_loss = loss(Y)\n",
    "        \n",
    "        left = np.argwhere(feat_values <= split_thresh).flatten()\n",
    "        right = np.argwhere(feat_values > split_thresh).flatten()\n",
    "        \n",
    "        if len(left) == 0 or len(right) == 0:\n",
    "            return 0\n",
    "#为啥返回0 呢？我也有点纳闷，增益为0？\n",
    "        \n",
    "        n = len(Y)\n",
    "        n_l,n_r = len(left),len(right)\n",
    "        e_l,e_r = loss(Y[left]),loss(Y[right])\n",
    "        child_loss = (n_l/n) * e_l + (n_r/n)*e_r\n",
    "        \n",
    "        ig = parent_loss -child_loss\n",
    "        \n",
    "        return ig\n",
    "# 这里的value是指的叶子节点？这个递归也也太多了吧\n",
    "    def _traverse(self,X,node,prob = False):\n",
    "        if isinstance(node,Leaf):\n",
    "            if self.classifier:\n",
    "                return node.value if prob else node.value.argmax()\n",
    "            return node.value\n",
    "        if X[node.feature] <= node.threshold:\n",
    "            return self._traverse(X,node.left,prob)\n",
    "        return self._traverse(X,node.right,prob)\n",
    "# 这个traversek看的我比较迷，表示需要这个吗\n",
    "# 看的有点那么懂了    \n",
    "\n",
    "    \n",
    "        #先从简单的代码进行处理，例如定义函数的评估函数\n",
    "    def mse(y):\n",
    "        return np.mean((y - np.mean(y))**2)\n",
    "\n",
    "    def entropy(y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist /np.sum(hist)\n",
    "        return  -np.sum([p*np.log2(p) for p in ps if p > 0])\n",
    "    def gini(y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist/np.sum(hist)\n",
    "        return 1- np.sum([i**2 for i in ps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 随机森林的算法实现\n",
    "\n",
    "由刚才决策树的实现代码来进行随机森林代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林的编写，用到了决策树\n",
    "\n",
    "import numpy as np\n",
    "from .dt import DecisionTree\n",
    "\n",
    "# 使用自主法进行有放回的抽样\n",
    "# 不过我对这个代码中为什么会取全量的抽样比较困惑，虽然那样选择也不是全量因为肯定是有对样本有重复的选择\n",
    "# 在写类的时候最大的感受是没有感受到面对过程时需要return什么，这点我自己不是那么理的清楚，更多的是写了很多方法之类的\n",
    "\n",
    "def bootstrap_sample(X,Y):\n",
    "    N,M = X.shape\n",
    "    idxs = np.random.choice(N,N,replace=True)\n",
    "    return X[idxs],Y[idxs]\n",
    "\n",
    "# 进行随机森林的算法\n",
    "class RandomForest:\n",
    "    def __init__(self,n_trees,max_depth,n_feats,classifier=True,criterion = \"enropy\")\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.n_feats = n_feats\n",
    "        self.classifier = classifier\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_tress):\n",
    "            X_samp, Y_samp = bootstrap_sample(X,Y)\n",
    "            tree = DecisionTree(\n",
    "                n_feats = self.n_feats,\n",
    "                max_depth=self.max_depth,\n",
    "                criterion = self.criterion,\n",
    "                classifier = self.classifier,\n",
    "            )\n",
    "            tree.fit(X_samp,Y_samp)\n",
    "            self.trees.append(tree)\n",
    "# 这个append的tree是append的啥 这点我自己其实有点懵\n",
    "# 下面那个调用我自己也有点弄的半懂半不懂的\n",
    "def predict(self,X):\n",
    "    tree_peds = np.array([[t._traverse(x,t.root) for x in X] for t in self.trees])\n",
    "    return self._vote(tree_preds)\n",
    "# 为什么要转置，表示有点懵\n",
    "def _vote(self,predictions):\n",
    "    if self.classifier:\n",
    "        out = [np.bincount(x).argmax() for x in predictions.T]\n",
    "    else:\n",
    "        out = [np.mean(x) for x in prediction.T]\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GBDT的算法实现\n",
    "\n",
    "鉴于GBDT的算法是对上面决策树和后面的XGBOOST，承上启下作用的算法，因此在这里对算法进行较为详细的讲解\n",
    "\n",
    "主要的参考资料是[GBDT原理介绍](https://www.csuldw.com/2019/07/12/2019-07-12-an-introduction-to-gbdt/),[GBDT最早的论文Friedman](https://statweb.stanford.edu/~jhf/ftp/trebst.pdf)和[知乎一文理解GBDT的原理](https://zhuanlan.zhihu.com/p/29765582) 和[陈天奇对于xgboost的解释](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n",
    "\n",
    "就像刚才讲到的一样GBDT的本质是，所有决策树的结果进行**加权求和**的运算来进行对函数进行逼近，其中模型的是我们的**CART树**，因为我们离散和连续的变量都要进行处理\n",
    "\n",
    "$\\sum_{k=1}^{K}f_k(X_i)$\n",
    "\n",
    "树的结构和树之间的处理方式都已经知道，参数就是决策树的那些参数，后面就是怎么样去评价每一次增加一颗数对我们的预测有多大的改善，所以我们需要引入**目标函数**其实我更愿意将之看成评估函数，因为本身就是对模型的参数进行评估和选择。\n",
    "\n",
    "$\n",
    "Obj^t \n",
    "=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t})\n",
    "\\\\=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t-1}+f_t(X_i))\n",
    "\\\\\\approx\\sum_{i=1}^{n}[l(y_i,\\hat{y}_i^{t-1})+\\partial_{\\hat{y}^{t-1}_{i}} l(y_i,\\hat{y}_i^{t-1})f_t(X_i)]\n",
    "$\n",
    "\n",
    "上面的目标函数就是我们对于新加入的树进行衡量的标准，其中上标t是指的第t颗树，t-1指的是前面t-1颗树。由于我们现在做的是对新加入对树进行衡量，所以前面t-1颗树或者说前面t-1轮的模型预测结果我们是已知的，因为我们这个目标函数其实就是看真实值和预测值的差距，所以这个目标函数需要尽量小，所谓的“误差小”，除去常数项后便可看到我们的**目标函数只依赖于每个数据点的在误差函数上的一阶导数**，所以这也是我们GBDT中梯度的由来，在陈天奇的xgboost直接将这个公式（当然是二阶的泰勒展开）添加了正则项对算法进行了进一步的改进，而**Friedman的论文是将一阶导的负数当成学习的函数来进行拟合求参数**，下面的代码也是按照那样来的。这点区别自己也可以注意下，虽然后面还有对GBDT的目标函数更进一步的公式简化和推导，但是以上公式是GBDT最为核心的思想，至于为什么用梯度，Friedman说这是最常用和最简单的方法，有点类似gini系数简化计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class ClassProbEstimator:\n",
    "    def fit(self,X,y):\n",
    "        self.class_prob = y.sum() / len(y)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        pred = np.empty(X.shape[0],dtype=np.float64)\n",
    "        pred.fill(self.class_prob)\n",
    "        return pred\n",
    "    \n",
    "class MeanBaseEstimator:\n",
    "    def fit(self,X,y):\n",
    "        self.avg = np.mean(y)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        pred = np.empty(X.shape[0],dtype=np.float64)\n",
    "        pred.fill(self.avg)\n",
    "        return pred\n",
    "# 加入了调用函数，将定义的类的实例也当成函数来\n",
    "# 但是坦白而言没有看到作者后面利用了这个啊，比较困惑那为啥要定义这个\n",
    "class MSELoss:\n",
    "    def __call__(self,y,y_pred):\n",
    "        return np.mean((y - y_pred)**2)\n",
    "        \n",
    "    def base_estimator(self):\n",
    "        return MeanBaseEstimator()\n",
    "# 这个是弄grad定义成这样主要是为了什么呢？这个我有点不是很理解对于我来说\n",
    "# 只能说稍微有点知道了，这个计算梯度和我们如何定义损失函数关，其实不是很知道为什么会有len(y),看系数有2应该就是平方和的感觉啊\n",
    "# 关于len(y)就是那个N的意思，计算的是一个样本的梯度，所以要弄一下\n",
    "    def grad(self,y,y_pred):\n",
    "        return -2 / len(y) * (y - y_pred)\n",
    "# 线性搜索也不是那么的懂，这个h_pred是什么意思\n",
    "# 这里的一个作用就是在调整步长，和作者所说的是调整weight来的有点出入\n",
    "# 这里的设定更多的是输入的时候对步长进行的调整，默认其实还是按照恒定的步长\n",
    "# 懵懵懂不是那么的清楚为什么，friedman的论文中有写到为什么\n",
    "    def line_search(self,y,y_pred,h_pred):\n",
    "        Lp = np.sum((y - y_pred) * h_pred)\n",
    "        Lpp = np.sum(h_pred * h_pred)\n",
    "        \n",
    "        return 1 if np.sum(Lpp) == 0 else Lp / Lpp\n",
    "# 这个eps也看不懂,是machine epsilon 可以理解为计算机在计算存储浮点数不准确，给了一个上限\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "    def __call__(self,y,y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -np.sum(y * np.log(y_pred + eps))\n",
    "     \n",
    "    def base_estimator(self):\n",
    "        return ClassProbEstimator()\n",
    "    \n",
    "    def grad(self,y,y_pred):\n",
    "        eps = np.finfo(float).eps\n",
    "        return -y* 1/(y_pred + eps)\n",
    "    \n",
    "    def line_search(self,y,y_pred,h_pred):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# 整个块为什么要这么定义我自己其实并不是很明白，为什么要这样去弄这样的结构\n",
    "\n",
    "\n",
    "# 定义好损失函数之后开始写GBDT的函数\n",
    "# y变量的one_hot编码\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .dt import DecisionTree\n",
    "from .losses import MSELoss,CrossEntropyLoss\n",
    "\n",
    "def to_one_hot(labels,n_classes=None):\n",
    "    if labels.ndm > 1:\n",
    "        raise ValueError(\"labels must have dimension 1,but got {}\".format(labels.ndim))\n",
    "        \n",
    "    N = labels.size\n",
    "    n_cols = np.max(labels) + 1 if n_classes is None else n_classes\n",
    "    one_hot = np.zeros((N,n_cols))\n",
    "    one_hot[np.arange(N),labels] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# 先上来就定义了一大段感觉也是醉了，这样看的话确实感觉有点尴尬面向对象的语言\n",
    "class GradientBoostedDecisionTree:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_iter,\n",
    "        max_depth = None,\n",
    "        classifier = True,\n",
    "        learning_rate = 1,\n",
    "        loss = \"crossentropy\",\n",
    "        step_size = \"constant\"\n",
    "    ):\n",
    "        self.loss = loss\n",
    "        self.weights = None\n",
    "        self.learners = None\n",
    "        self.out_dims = None\n",
    "        self.n_iter = n_iter\n",
    "        self.base_estimator = None\n",
    "        self.max_depth = max_depth\n",
    "        self.step_size = step_size\n",
    "        self.classifier = classifier\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def fit(self,X,Y):\n",
    "        if self.loss == \"mse\":\n",
    "            loss = MSELoss()\n",
    "        elif self.loss == \"crossentropy\":\n",
    "            loss = CrossEntropyLoss()\n",
    "# 后面那个为什么要那样的变换，这点我不是很懂为啥\n",
    "# 这里是对Y进行的变换，如果是分类的话进行one——hot编码，如果是连续变量且纬度为1的话就变成列变量\n",
    "        if self.classifier:\n",
    "            Y = to_one_hot(Y.flatten())\n",
    "        else:\n",
    "            Y = Y.reshape(-1,1) if len(Y.shape) == 1 else Y\n",
    "    \n",
    "        N, M = X.shape\n",
    "        self.out_dims = Y.shape[1]\n",
    "        self.learners = np.empty((self.n_iter,self.out_dims),dtype=object)\n",
    "        self.weights = np.ones((self.n_iter,self.out_dims))\n",
    "        self.weights[1:,:] *= self.learning_rate\n",
    "# 下面这些也知道是什么但是还是不懂。。。。。   \n",
    "        Y_pred = np.zeros((N,self.out_dims))\n",
    "        for k in range(self.out_dims):\n",
    "            t = loss.base_estimator()\n",
    "            t.fit(X,Y[:,k])\n",
    "            Y_pred[:,k] += t.predict(X)\n",
    "            self.learners[0,k] = t\n",
    "\n",
    "        for i in range(1,self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                y,y_pred = Y[:,k],Y_pred[:,k]\n",
    "                neg_grad = -1 * loss.grad(y,y_pred)\n",
    "\n",
    "                t = DecisionTree(\n",
    "                   classifier = False,max_depth=self.max_depth,criterion=\"mse\"\n",
    "                )\n",
    "#这里的fit中的y就是指的负梯度的方向，这也算是最为精华的部分了，或者点题的部分\n",
    "                t.fit(X,neg_grad)\n",
    "                self.learners[i,k] = t\n",
    "# \n",
    "                step = 1.0\n",
    "                h_pred = t.predict(X)\n",
    "                if self.step_size == \"adaptive\":\n",
    "                    step = loss.line_search(y,y_pred,h_pred)\n",
    "\n",
    "                self.weight[i,k] *= step\n",
    "                Y_pred[:,k] += self.weight[i,k]*h_pred\n",
    "\n",
    "        \n",
    "    def predict(self,X):\n",
    "        Y_pred = np.zeros((X.shape[0],self.out_dims))\n",
    "        for i in range(self.n_iter):\n",
    "            for k in range(self.out_dims):\n",
    "                Y_pred[:,k] += self.weights[i,k] * self.learners[i,k].predict(X)\n",
    "                \n",
    "        if self.classifier:\n",
    "            Y_pred = Y_pred.argmax(axis = 1)\n",
    "            \n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XBGBOOST的算法实现\n",
    "\n",
    "主要参考资料为印象笔记中陈天奇的[XGBoost 与 Boosted Tree](https://app.yinxiang.com/shard/s43/nl/8933006/3684c27a-1e8d-4c6a-b7f0-8c56a7ca057b/)的文章，这篇文章主要是来源于其在华盛顿大学助教的讲义[陈天奇对于xgboost的解释](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n",
    "同时[Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)xgboost的说明文档也做了就是说明\n",
    "\n",
    "XGBOOST的理论基础是上面GBDT理论的延续，我们对GBDT的目标函数定义如下\n",
    "\n",
    "$\n",
    "Obj^t \n",
    "=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t})\n",
    "\\\\=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t-1}+f_t(X_i))\n",
    "\\\\\\approx\\sum_{i=1}^{n}[l(y_i,\\hat{y}_i^{t-1})+\\partial_{\\hat{y}^{t-1}_{i}} l(y_i,\\hat{y}_i^{t-1})f_t(X_i)]\n",
    "$\n",
    "\n",
    "xgboost在此的基础上增加了二阶的泰勒展开和正则项的引入\n",
    "\n",
    "$\n",
    "Obj^t \n",
    "=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t}) + \\sum_{i=1}^{t}\\Omega(f_{i})\n",
    "\\\\=\\sum_{i=1}^{n}l(y_i,\\hat{y}_i^{t-1}+f_t(X_i)) + \\Omega(f_{t}) + constant\n",
    "\\\\\\approx\\sum_{i=1}^{n}[l(y_i,\\hat{y}_i^{t-1})+\\partial_{\\hat{y}^{t-1}_{i}} l(y_i,\\hat{y}_i^{t-1})f_t(X_i)+\\frac{1}{2}\\partial_{\\hat{y}^{t-1}_{i}}^2l(y_i,\\hat{y}^{t-1})f_t^2(x_i)] +\\Omega(f_{t}) + constant\n",
    "$\n",
    "\n",
    "等我们消除常数之后便会得到\n",
    "\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{n}[g_if_t(X_i)+\\frac{1}{2}h_if_t^2(x_i)] +\\Omega(f_{t})\n",
    "$\n",
    "\n",
    "$\n",
    "\\\\g_i = \\partial_{\\hat{y}^{t-1}_{i}}l(y_i,\\hat{y}_i^{t-1}),\n",
    "\\\\h_i = \\partial_{\\hat{y}^{t-1}_{i}}^2l(y_i,\\hat{y}^{t-1})\n",
    "$\n",
    "\n",
    "**这就是目前我们经过简化之后的目标函数，不过还没有对复杂度进行定义**\n",
    "\n",
    "按照陈天奇的定义\n",
    "> 这个复杂度包含了一棵树里面节点的个数，以及每个树叶子节点上面输出分数的$L2$模平方。\n",
    "\n",
    "其中\n",
    "$\n",
    "f_t(x) = w_q(x), w\\in R^T,q:R^d \\to\\{1,2,3,4,...,T\\}\n",
    "$\n",
    "直白点的话就是，q(x)指的是输入映射到叶子的索引号，w_i指的是对应的索引号的分数。T是节点的个数\n",
    "$f_t(x)$ 是叶子节点的分数向量\n",
    "\n",
    "![定义树](xgboost.png)\n",
    "\n",
    "$\n",
    "\\Omega(f_{t})=\\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2\n",
    "$\n",
    "\n",
    "另外引入每个子叶子上样本的集合\n",
    "$\n",
    "I_j = \\{i|q(x_i)=j\\}\n",
    "$\n",
    "\n",
    "所以目标函数可以改写为，陈认为这部**最为关键**\n",
    "\n",
    "$\n",
    "Obj^{(t)} \\approx \\sum_{i=1}^{n}[g_if_t(X_i)+\\frac{1}{2}h_if_t^2(x_i)] +\\Omega(f_{t})\n",
    "\\\\=\\sum_{i=1}^{n}[g_iw_{q(x_i)}+\\frac{1}{2}h_iw_{q(x_i)}^2] +\\gamma T + \\frac{1}{2}\\lambda\\sum_{j=1}^{T}w_j^2\n",
    "\\\\=\\sum_{j=1}^{T}[(\\sum_{i\\in I_j}g_i)w_j+\\frac{1}{2}(\\sum_{i\\in I_j}h_i+\\lambda)w_j^2] + \\gamma T\n",
    "$\n",
    "\n",
    "从上面的公式便可以看出，这里做了个转换也就是将每个要计算的数据直接放在叶节点进行计算，从数据输入的角度到最后叶节点的角度\n",
    "\n",
    "按照陈的说法就是这个目标函数包含了T个相互独立的变量，定义\n",
    "\n",
    "$G_j = \\sum_{i\\in I_j}g_i$\n",
    "\n",
    "$H_j = \\sum_{i\\in I_j}h_i$\n",
    "\n",
    "那目标函数便可改写为\n",
    "$\n",
    "Obj^{(t)} \\approx\\sum_{j=1}^{T}[(\\sum_{i\\in I_j}g_i)w_j+\\frac{1}{2}(\\sum_{i\\in I_j}h_i+\\lambda)w_j^2] + \\gamma T\n",
    "\\\\=\\sum_{j=1}^T[G_jW_j+\\frac{1}{2}(H_j+\\lambda)w_j^2)]+\\gamma T\n",
    "$\n",
    "\n",
    "也就说我们经过一系列转换之后便得到了简化后的目标函数为了求这个函数的最小值，首先我们要知道的是我们求的是Wj的最有，所以我们直接对目标函数wj的一阶导数等于0，便可得出\n",
    "\n",
    "$\n",
    "\\sum_{j=1}^T[G_j+(H_j+\\lambda)w_j)]=0\n",
    "$\n",
    "\n",
    "所以我们最后就得出了最优解和对应的目标函数\n",
    "\n",
    "$\n",
    "W_j^* = -\\frac{G_j}{(H_j+\\lambda)}\n",
    "\\\\Obj = -\\frac{1}{2}\\sum_{j=1}^{T}\\frac{G_j^2}{H_j + \\lambda} + \\gamma T\n",
    "$\n",
    "\n",
    "对应的目标函数就像我们的打分函数一样，类似熵和gini系数一样对我们新产生的数进行评估，下图是简单的例子\n",
    "![The Structure Score Calculation](score.png)\n",
    "\n",
    "具体到实际的操作环节，主要是用贪心算法对已有的叶子加入一个分割，因为我们用的是CART树，所以我们是按照二叉树那样分割的，计算分割前后的增益\n",
    "\n",
    "$\n",
    "Gain = -\\frac{1}{2}\\frac{G_L+^2+G_R^2}{H_L+H_R+\\lambda}+\\gamma-(-\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}] + 2\\gamma) \n",
    "\\\\=\\frac{1}{2}[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{G_L+^2+G_R^2}{H_L+H_R+\\lambda}] -\\gamma\n",
    "$\n",
    "\n",
    "这也是我们讲的xgboost与gbdt的主要区别是引入了损失函数的二阶展开和正则项\n",
    "\n",
    "总结到这里我忽然有疑问既然已经有了熵，gini，mse的评价手段，那为什么还需要梯度来进行函数的优化呢？\n",
    "\n",
    "这里gini是针对离散变量，cart树输出的是分数所以我们用的是基于连续值的输入进行判定，所以会用mse来衡量，mse只是我们函数的一种特殊形式，我们这个时候定义的目标函数更具有一般性，就像陈所说的\n",
    "> 这样一个形式包含可所有可以求导的目标函数。也就是说有了这个形式，我们写出来的代码可以用来求解包括回归，分类和排序的各种问题，正式的推导可以使得机器学习的工具更加一般\n",
    "\n",
    "当然他指的是下面的推导结果\n",
    "\n",
    "$\n",
    "\\sum_{i=1}^{n}[g_if_t(X_i)+\\frac{1}{2}h_if_t^2(x_i)] +\\Omega(f_{t})\n",
    "$\n",
    "\n",
    "$\n",
    "\\\\g_i = \\partial_{\\hat{y}^{t-1}_{i}}l(y_i,\\hat{y}_i^{t-1}),\n",
    "\\\\h_i = \\partial_{\\hat{y}^{t-1}_{i}}^2l(y_i,\\hat{y}^{t-1})\n",
    "$\n",
    "\n",
    "\n",
    "最后一步的讲解[知乎这篇](https://zhuanlan.zhihu.com/p/29765582)讲的比较清楚\n",
    "\n",
    "接下来主要就讲算法，由于xgboost源库中的代码超过我自身的能力的范围，而且本文档的主要目的就是对算法的理论和基本实现进行介绍所以没有必要进行深入的介绍\n",
    "\n",
    "代码实现主要参考知乎的这篇文章[XGBoost的python源码实现](https://zhuanlan.zhihu.com/p/32181687) 当然这个代码的有很多问题但是就基本的核心功能算是实现了，不过具体的评判过程之类的没有那么详细"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "import numpy as np\n",
    "import progressbar \n",
    "\n",
    "from decision_tree.decision_tree_model import DecisionTree\n",
    "from utils.misc import bar_widgets\n",
    "#这里是对问题的简化，直接定义了均方误差为损失的类，计算其一阶导数二阶导数\n",
    "#不过感觉这个一阶求导是有问题的，应该是有负号存在的，是对predicted求导\n",
    "class LeastSquaresLoss():\n",
    "    def gradient(self,actual,predicted):\n",
    "        return actual - predicted\n",
    "    def hess(self,actual,predicted):\n",
    "        return np.ones_like(actual)\n",
    "    \n",
    "class XGboostRegressionTree(DecisionTree):\n",
    "    def _split(self,y):\n",
    "        col = int(np.shape(y)[1]/2)\n",
    "        y,y_pred = y[:,:col],y[:,col:]\n",
    "        return y,y_pred\n",
    "    \n",
    "    def _gain(self,y,y_pred):\n",
    "        nominator = np.power((self.loss.gradient(y,y_pred)).sum(),2)\n",
    "        denominator = self.loss.hess(y,y_pred).sum()\n",
    "        return 0.5*(nominator/denominator)\n",
    "#这里的增益算法没有1/2的系数和gamma的正则项    \n",
    "    def _gian_by_taylor(self,y,y1,y2)\n",
    "        y,y_pred = self._split(y)\n",
    "        y1,y1_pred = self._split(y1)\n",
    "        y2,y2_pred = self._split(y2)\n",
    "        \n",
    "        true_gain = self._gain(y1,y1_pred)\n",
    "        false_gain = self._gain(y2,y2_pred)\n",
    "        gain = self._gain(y,y_pred)\n",
    "        return true_gain + false_gain - gain\n",
    "#这里又省略了wi公式中的负号和分母中的lambda\n",
    "    def _approximate_update(self,y):\n",
    "        y,y_pred = self._split(y)\n",
    "        gradient = np.sum(self.loss.gradient(y,y_pred),axis=0)\n",
    "        hessian = np.sum(self.loss.hess(y,y_pred),axis = 0)\n",
    "        update_approximation = gradient / hessian\n",
    "        return update_approximation\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self._impurity_calculation = self._gain_by_tayor\n",
    "        self._leaf_value_calculation = self._approximate_update\n",
    "        super(XGBoostRegressionTree,self).fit(X,y)\n",
    "        \n",
    "class XGBoost(object):\n",
    "    def __init__(self,n_estimators=200,learning_rate=0.01,min_samples_split=2,min_impurity=1e-7,max_depth=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity = min_impurity\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.bar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "        \n",
    "        self.loss = LeastSquaresLoss()\n",
    "        \n",
    "        self.tresss = []\n",
    "        \n",
    "        for _ in range(n_estimators):\n",
    "            tree = XGBoostRegressionTree(\n",
    "                    min_samples_split = self.min_samples_split,\n",
    "                    min_impurity = min_impurity,\n",
    "                    max_depth = self.max_depth,\n",
    "                    loss = self.loss)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            \n",
    "    def fix(self,X,y):\n",
    "        m = X.shape[0]\n",
    "        y = np.reshape(y,(m,-1))\n",
    "        y_pred = np.zeros(np.shape(y))\n",
    "        for i in self.bar(range(self.n_estimators)):\n",
    "            tree = self.trees[i]\n",
    "            y_and_pred = np.concatenate((y,y_pred),axis=1)\n",
    "            tree.fit(X,y_and_pred)\n",
    "            update_pred = tree.predict(X)\n",
    "            update_pred = np.reshape(update_pred,(m,-1))\n",
    "            y_pred += update_pred\n",
    "            \n",
    "    def predict(self,X):\n",
    "        y_pred = None\n",
    "        m = X.shape[0]\n",
    "        for tree in self.trees:\n",
    "            update_pred = tree.predict(X)\n",
    "            update_pred = np.reshape(update_pred,(m,-1))\n",
    "            if y_pred is None:\n",
    "                y_pred = np.zeros_like(update_pred)\n",
    "            y_pred += update_pred\n",
    "            \n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此便完成了决策树的理论和算法实现，对于底层数学原理的理解有助于对现有的算法进行更加深入的理解，例如那些lightgbm之类在这里都没有讲，个人是觉得适可而止，前面讲的算法也基本覆盖真个决策树的主线。这也是之前我没有做好的事情。后面个人觉得可以按照这种主题式的方式进行学习"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
